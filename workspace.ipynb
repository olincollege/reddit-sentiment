{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# literally a space to work on the writeup while avoiding merge conflicts. put this in the other notebook when done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Sentiment Analysis\n",
    "## Allison Li and Berwin Lan\n",
    "### Software Design Spring 2021 Midterm Project\n",
    "\n",
    "## Introduction\n",
    "In this project, we wanted to investigate the nature of human behavior and self-regulation in anonymous virtual spaces. Reddit is an online forum where replies are nested in threads; that is, a single comment can generate many replies as child comments, and each of those comments can then generate its own respective child comments. We were interested in how sentiments, or emotion, changed throughout a comment thread, and whether sentiment was amplified or damped by increased nesting. This story is important because of the differences seen in online vs. in-person communication, and our results can help us understand what drives the way people behave and interact in online spaces, especially when anonymous. Our project is organized into data processing in `data_cleaning.py`, sentiment analysis in `sentiment_analysis.py`, and data visualization in this notebook. We acquired data, cleaned it, analyzed it, and visualized it. In this computational essay, we offer our conclusions and a look into our process.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "We used data from Reddit, which we scraped using [PRAW (Python Reddit API Wrapper)](https://pypi.org/project/praw \"Allows for simple access to reddit's API.\") to access the [Reddit API](https://www.reddit.com/wiki/api \"Reddit API Access.\"). The data is directly stored in .csv files, which are accessed using [pandas](https://pandas.pydata.org/ \"A data analysis tool\") DataFrames. Our visualizations, created using [seaborn](https://seaborn.pydata.org/) and [matplotlib](https://matplotlib.org/) use the subreddit in which a thread is located, the depth of each individual comment, and the average sentiment of all the comments at a given depth in a given comment thread.\n",
    "\n",
    "### Data Processing\n",
    "Data processing is comprised of three steps: cleaning the data, tokenizing the data, and lemmatizing the data. \n",
    "* Data cleaning is the process of removing all unwanted characters, such as links and emojis.\n",
    "\n",
    "[insert example here]\n",
    "\n",
    "* Tokenizing the data breaks a block of text into parts; in our case, we tokenized each paragraph of input into sentences. \n",
    "\n",
    "[insert example here]\n",
    "\n",
    "* Lemmatizing is the process by which different inflections of a word are collapsed into its base form for analysis. For example, the words `coding`, `codes`, and `coder` would all be considered `code` after being lemmatized.\n",
    "\n",
    "[insert example here]\n",
    "\n",
    "### Sentiment Analysis\n",
    "A general, top-level view of sentiment analysis can be described as follows:\n",
    "* Find and store all the child replies to a single comment.\n",
    "* Organize comments in a thread by depth, where the original comment has a depth of `0`, its replies have a depth of `1`, etc.\n",
    "* Select the comments with the greatest depth to analyze.\n",
    "* Find the average sentiment of all comments at one depth, and create a dictionary of each depth's average sentiment.\n",
    "\n",
    "Please refer to `sentiment_analysis.py` for greater detail.\n",
    "\n",
    "## Results\n",
    "\n",
    "    What summaries or visualizations did you create?\n",
    "    What are the interesting and/or important parts of these summaries or visualizations?\n",
    "    How do these results answer your questions or tell your story?\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "    What are the important insights that the reader should get from this project?\n",
    "    * Our project is a very limited, preliminary examination of how the nature of online interaction shapes the emotional content of messages. While we set out to investigate human behavior and how it is impacted by online spaces and anonymity, our small sample size means that any conclusions that can be drawn or trends that can be identified should be regarded with a significant degree of skepticism. Based off\n",
    "    What are the contextual or ethical implications of your topic or work?\n",
    "    * \n",
    "    What lessons did you learn as you did the project?\n",
    "    * In plotting, we learned that there was a non-negligible amount of data processing that had to occur to get meaningful plots.\n",
    "    What were the most difficult, challenging, or frustrating parts of the project?\n",
    "    * Cleaning the data in preparation for sentiment analysis was challenging, as there were some inconsistencies in the documentation and resources we were using: for example, we needed to refactor our code to analyze entire sentences rather than individual words. Referring directly to the VADER documentation would have saved us time here. Writing unit tests was also difficult, due to the size and complexity of both the data and VADER/NLTK. \n",
    "    In what ways would you extend or change your project if you had more time?\n",
    "    * The next steps of this project would be to gather more data points, both in terms of the length of comment threads and the number of unique contributors at each depth. With a larger sample size, our data can be more reliable and the conlusions more concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
